# veritying python version
# !python --version

# !pip install pyspark

# using spark and veritying its version
from pyspark.sql import SparkSession

spark = SparkSession \
                    .builder \
                    .appName('Eng. dados - Tecnologia de Big Data - aula 2.2') \
                    .getOrCreate()

spark.version

filename = "/content/sample_data/numbers.txt" # precisa referenciar a estrutura de diretorios do google colab, q eh linux
text_df = spark.read.text(filename)
# Apply a transformation to extract the first element of each row
linesRdd = text_df.rdd.map(lambda r: r[0]) # rdd: estrutura de dado do python q abstrai o cluster, como se estivesse na maq local
print('Number of partitions: {}'.format(linesRdd.getNumPartitions()))

# Collect the results (for demonstration purposes)
# result = linesRdd.collect()

# Display the results
# print(result)

"""
The objective of code below is to count how many times each number was showed in the file
Sample file:
  1 9 8 10 1 9 2
Result:
  1 -> 2  # 2 times
  9 -> 2
  8 -> 1
  10 -> 1
  2 -> 1
"""
from operator import add

"""
linesRdd.sample(False, 0.01): only 1% of the lines for test
flatMap: ?
filter: only pair numbers
map & reduce: responsible for counting; add 1 to every equal number
"""
countsRdd = linesRdd.sample(False, 0.01) \
                    .flatMap(lambda line: line.split(' ')) \
                    .filter(lambda number: int(number)%2==0) \
                    .map(lambda number: (number, 1)) \
                    .reduceByKey(add)

print('The end')

output = countsRdd.collect()
for(number, count) in output:
  print(number, count)

linesRdd.count() # total lines of file (not from the sample/amostra)