{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnwkSGjTJgqYM04YnDsiDJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elbyvaz/data_engineering/blob/main/spark/spark_readme_file.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzPWa1A7t0U7",
        "outputId": "c38e619b-e6d8-4708-c949-767db8ebb0dc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=cafece1c186eb5f5c61ec4564af05f984644df005dd71d51318281f1a81cce4d\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "6K_gmD5-s6Ku",
        "outputId": "b5bfa2a4-aa94-41d1-abac-e3797f1e1bac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.5.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# using spark and veritying its version\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession \\\n",
        "                    .builder \\\n",
        "                    .appName(\"Readme exercise\") \\\n",
        "                    .getOrCreate()\n",
        "\n",
        "spark.version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# count total lines of the file\n",
        "filename = \"/content/sample_data/README.md\"\n",
        "linesRdd = spark.sparkContext.textFile(filename)\n",
        "linesRdd.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDt3JwqEuTyj",
        "outputId": "89ddb115-8784-4321-edab-eb337139589a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "125"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# map\n",
        "mapRdd = linesRdd.map(lambda line: (line, len(line))) # tuple: each line will have the line content and its length\n",
        "mapRdd.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHTiHTzZuUGV",
        "outputId": "857e7592-f2ba-4fbc-bbc8-d05ba2f40028"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('# Apache Spark', 14),\n",
              " ('', 0),\n",
              " ('Spark is a unified analytics engine for large-scale data processing. It provides',\n",
              "  80),\n",
              " ('high-level APIs in Scala, Java, Python, and R, and an optimized engine that',\n",
              "  75),\n",
              " ('supports general computation graphs for data analysis. It also supports a',\n",
              "  73),\n",
              " ('rich set of higher-level tools including Spark SQL for SQL and DataFrames,',\n",
              "  74),\n",
              " ('pandas API on Spark for pandas workloads, MLlib for machine learning, GraphX for graph processing,',\n",
              "  98),\n",
              " ('and Structured Streaming for stream processing.', 47),\n",
              " ('', 0),\n",
              " ('<https://spark.apache.org/>', 27),\n",
              " ('', 0),\n",
              " ('[![GitHub Actions Build](https://github.com/apache/spark/actions/workflows/build_main.yml/badge.svg)](https://github.com/apache/spark/actions/workflows/build_main.yml)',\n",
              "  167),\n",
              " ('[![AppVeyor Build](https://img.shields.io/appveyor/ci/ApacheSoftwareFoundation/spark/master.svg?style=plastic&logo=appveyor)](https://ci.appveyor.com/project/ApacheSoftwareFoundation/spark)',\n",
              "  189),\n",
              " ('[![PySpark Coverage](https://codecov.io/gh/apache/spark/branch/master/graph/badge.svg)](https://codecov.io/gh/apache/spark)',\n",
              "  123),\n",
              " ('[![PyPI Downloads](https://static.pepy.tech/personalized-badge/pyspark?period=month&units=international_system&left_color=black&right_color=orange&left_text=PyPI%20downloads)](https://pypi.org/project/pyspark/)',\n",
              "  210),\n",
              " ('', 0),\n",
              " ('', 0),\n",
              " ('## Online Documentation', 23),\n",
              " ('', 0),\n",
              " ('You can find the latest Spark documentation, including a programming', 68),\n",
              " ('guide, on the [project web page](https://spark.apache.org/documentation.html).',\n",
              "  78),\n",
              " ('This README file only contains basic setup instructions.', 56),\n",
              " ('', 0),\n",
              " ('## Building Spark', 17),\n",
              " ('', 0),\n",
              " ('Spark is built using [Apache Maven](https://maven.apache.org/).', 63),\n",
              " ('To build Spark and its example programs, run:', 45),\n",
              " ('', 0),\n",
              " ('```bash', 7),\n",
              " ('./build/mvn -DskipTests clean package', 37),\n",
              " ('```', 3),\n",
              " ('', 0),\n",
              " ('(You do not need to do this if you downloaded a pre-built package.)', 67),\n",
              " ('', 0),\n",
              " ('More detailed documentation is available from the project site, at', 66),\n",
              " ('[\"Building Spark\"](https://spark.apache.org/docs/latest/building-spark.html).',\n",
              "  77),\n",
              " ('', 0),\n",
              " ('For general development tips, including info on developing Spark using an IDE, see [\"Useful Developer Tools\"](https://spark.apache.org/developer-tools.html).',\n",
              "  157),\n",
              " ('', 0),\n",
              " ('## Interactive Scala Shell', 26),\n",
              " ('', 0),\n",
              " ('The easiest way to start using Spark is through the Scala shell:', 64),\n",
              " ('', 0),\n",
              " ('```bash', 7),\n",
              " ('./bin/spark-shell', 17),\n",
              " ('```', 3),\n",
              " ('', 0),\n",
              " ('Try the following command, which should return 1,000,000,000:', 61),\n",
              " ('', 0),\n",
              " ('```scala', 8),\n",
              " ('scala> spark.range(1000 * 1000 * 1000).count()', 46),\n",
              " ('```', 3),\n",
              " ('', 0),\n",
              " ('## Interactive Python Shell', 27),\n",
              " ('', 0),\n",
              " ('Alternatively, if you prefer Python, you can use the Python shell:', 66),\n",
              " ('', 0),\n",
              " ('```bash', 7),\n",
              " ('./bin/pyspark', 13),\n",
              " ('```', 3),\n",
              " ('', 0),\n",
              " ('And run the following command, which should also return 1,000,000,000:',\n",
              "  70),\n",
              " ('', 0),\n",
              " ('```python', 9),\n",
              " ('>>> spark.range(1000 * 1000 * 1000).count()', 43),\n",
              " ('```', 3),\n",
              " ('', 0),\n",
              " ('## Example Programs', 19),\n",
              " ('', 0),\n",
              " ('Spark also comes with several sample programs in the `examples` directory.',\n",
              "  74),\n",
              " ('To run one of them, use `./bin/run-example <class> [params]`. For example:',\n",
              "  74),\n",
              " ('', 0),\n",
              " ('```bash', 7),\n",
              " ('./bin/run-example SparkPi', 25),\n",
              " ('```', 3),\n",
              " ('', 0),\n",
              " ('will run the Pi example locally.', 32),\n",
              " ('', 0),\n",
              " ('You can set the MASTER environment variable when running examples to submit',\n",
              "  75),\n",
              " ('examples to a cluster. This can be a mesos:// or spark:// URL,', 62),\n",
              " ('\"yarn\" to run on YARN, and \"local\" to run', 41),\n",
              " ('locally with one thread, or \"local[N]\" to run locally with N threads. You',\n",
              "  73),\n",
              " ('can also use an abbreviated class name if the class is in the `examples`',\n",
              "  72),\n",
              " ('package. For instance:', 22),\n",
              " ('', 0),\n",
              " ('```bash', 7),\n",
              " ('MASTER=spark://host:7077 ./bin/run-example SparkPi', 50),\n",
              " ('```', 3),\n",
              " ('', 0),\n",
              " ('Many of the example programs print usage help if no params are given.', 69),\n",
              " ('', 0),\n",
              " ('## Running Tests', 16),\n",
              " ('', 0),\n",
              " ('Testing first requires [building Spark](#building-spark). Once Spark is built, tests',\n",
              "  84),\n",
              " ('can be run using:', 17),\n",
              " ('', 0),\n",
              " ('```bash', 7),\n",
              " ('./dev/run-tests', 15),\n",
              " ('```', 3),\n",
              " ('', 0),\n",
              " ('Please see the guidance on how to', 33),\n",
              " ('[run tests for a module, or individual tests](https://spark.apache.org/developer-tools.html#individual-tests).',\n",
              "  110),\n",
              " ('', 0),\n",
              " ('There is also a Kubernetes integration test, see resource-managers/kubernetes/integration-tests/README.md',\n",
              "  105),\n",
              " ('', 0),\n",
              " ('## A Note About Hadoop Versions', 31),\n",
              " ('', 0),\n",
              " ('Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported',\n",
              "  77),\n",
              " ('storage systems. Because the protocols have changed in different versions of',\n",
              "  76),\n",
              " ('Hadoop, you must build Spark against the same version that your cluster runs.',\n",
              "  77),\n",
              " ('', 0),\n",
              " ('Please refer to the build documentation at', 42),\n",
              " ('[\"Specifying the Hadoop Version and Enabling YARN\"](https://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn)',\n",
              "  157),\n",
              " ('for detailed guidance on building for a particular distribution of Hadoop, including',\n",
              "  84),\n",
              " ('building for particular Hive and Hive Thriftserver distributions.', 65),\n",
              " ('', 0),\n",
              " ('## Configuration', 16),\n",
              " ('', 0),\n",
              " ('Please refer to the [Configuration Guide](https://spark.apache.org/docs/latest/configuration.html)',\n",
              "  98),\n",
              " ('in the online documentation for an overview on how to configure Spark.',\n",
              "  70),\n",
              " ('', 0),\n",
              " ('## Contributing', 15),\n",
              " ('', 0),\n",
              " ('Please review the [Contribution to Spark guide](https://spark.apache.org/contributing.html)',\n",
              "  91),\n",
              " ('for information on how to get started contributing to the project.', 66)]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# flatMap: breaking content line in words\n",
        "flatMapRdd = linesRdd.flatMap(lambda line: line.split())\n",
        "flatMapRdd.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-Im_U94uUOO",
        "outputId": "f33891fd-ff5b-4895-c179-9b8c821cb8e4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['#',\n",
              " 'Apache',\n",
              " 'Spark',\n",
              " 'Spark',\n",
              " 'is',\n",
              " 'a',\n",
              " 'unified',\n",
              " 'analytics',\n",
              " 'engine',\n",
              " 'for',\n",
              " 'large-scale',\n",
              " 'data',\n",
              " 'processing.',\n",
              " 'It',\n",
              " 'provides',\n",
              " 'high-level',\n",
              " 'APIs',\n",
              " 'in',\n",
              " 'Scala,',\n",
              " 'Java,',\n",
              " 'Python,',\n",
              " 'and',\n",
              " 'R,',\n",
              " 'and',\n",
              " 'an',\n",
              " 'optimized',\n",
              " 'engine',\n",
              " 'that',\n",
              " 'supports',\n",
              " 'general',\n",
              " 'computation',\n",
              " 'graphs',\n",
              " 'for',\n",
              " 'data',\n",
              " 'analysis.',\n",
              " 'It',\n",
              " 'also',\n",
              " 'supports',\n",
              " 'a',\n",
              " 'rich',\n",
              " 'set',\n",
              " 'of',\n",
              " 'higher-level',\n",
              " 'tools',\n",
              " 'including',\n",
              " 'Spark',\n",
              " 'SQL',\n",
              " 'for',\n",
              " 'SQL',\n",
              " 'and',\n",
              " 'DataFrames,',\n",
              " 'pandas',\n",
              " 'API',\n",
              " 'on',\n",
              " 'Spark',\n",
              " 'for',\n",
              " 'pandas',\n",
              " 'workloads,',\n",
              " 'MLlib',\n",
              " 'for',\n",
              " 'machine',\n",
              " 'learning,',\n",
              " 'GraphX',\n",
              " 'for',\n",
              " 'graph',\n",
              " 'processing,',\n",
              " 'and',\n",
              " 'Structured',\n",
              " 'Streaming',\n",
              " 'for',\n",
              " 'stream',\n",
              " 'processing.',\n",
              " '<https://spark.apache.org/>',\n",
              " '[![GitHub',\n",
              " 'Actions',\n",
              " 'Build](https://github.com/apache/spark/actions/workflows/build_main.yml/badge.svg)](https://github.com/apache/spark/actions/workflows/build_main.yml)',\n",
              " '[![AppVeyor',\n",
              " 'Build](https://img.shields.io/appveyor/ci/ApacheSoftwareFoundation/spark/master.svg?style=plastic&logo=appveyor)](https://ci.appveyor.com/project/ApacheSoftwareFoundation/spark)',\n",
              " '[![PySpark',\n",
              " 'Coverage](https://codecov.io/gh/apache/spark/branch/master/graph/badge.svg)](https://codecov.io/gh/apache/spark)',\n",
              " '[![PyPI',\n",
              " 'Downloads](https://static.pepy.tech/personalized-badge/pyspark?period=month&units=international_system&left_color=black&right_color=orange&left_text=PyPI%20downloads)](https://pypi.org/project/pyspark/)',\n",
              " '##',\n",
              " 'Online',\n",
              " 'Documentation',\n",
              " 'You',\n",
              " 'can',\n",
              " 'find',\n",
              " 'the',\n",
              " 'latest',\n",
              " 'Spark',\n",
              " 'documentation,',\n",
              " 'including',\n",
              " 'a',\n",
              " 'programming',\n",
              " 'guide,',\n",
              " 'on',\n",
              " 'the',\n",
              " '[project',\n",
              " 'web',\n",
              " 'page](https://spark.apache.org/documentation.html).',\n",
              " 'This',\n",
              " 'README',\n",
              " 'file',\n",
              " 'only',\n",
              " 'contains',\n",
              " 'basic',\n",
              " 'setup',\n",
              " 'instructions.',\n",
              " '##',\n",
              " 'Building',\n",
              " 'Spark',\n",
              " 'Spark',\n",
              " 'is',\n",
              " 'built',\n",
              " 'using',\n",
              " '[Apache',\n",
              " 'Maven](https://maven.apache.org/).',\n",
              " 'To',\n",
              " 'build',\n",
              " 'Spark',\n",
              " 'and',\n",
              " 'its',\n",
              " 'example',\n",
              " 'programs,',\n",
              " 'run:',\n",
              " '```bash',\n",
              " './build/mvn',\n",
              " '-DskipTests',\n",
              " 'clean',\n",
              " 'package',\n",
              " '```',\n",
              " '(You',\n",
              " 'do',\n",
              " 'not',\n",
              " 'need',\n",
              " 'to',\n",
              " 'do',\n",
              " 'this',\n",
              " 'if',\n",
              " 'you',\n",
              " 'downloaded',\n",
              " 'a',\n",
              " 'pre-built',\n",
              " 'package.)',\n",
              " 'More',\n",
              " 'detailed',\n",
              " 'documentation',\n",
              " 'is',\n",
              " 'available',\n",
              " 'from',\n",
              " 'the',\n",
              " 'project',\n",
              " 'site,',\n",
              " 'at',\n",
              " '[\"Building',\n",
              " 'Spark\"](https://spark.apache.org/docs/latest/building-spark.html).',\n",
              " 'For',\n",
              " 'general',\n",
              " 'development',\n",
              " 'tips,',\n",
              " 'including',\n",
              " 'info',\n",
              " 'on',\n",
              " 'developing',\n",
              " 'Spark',\n",
              " 'using',\n",
              " 'an',\n",
              " 'IDE,',\n",
              " 'see',\n",
              " '[\"Useful',\n",
              " 'Developer',\n",
              " 'Tools\"](https://spark.apache.org/developer-tools.html).',\n",
              " '##',\n",
              " 'Interactive',\n",
              " 'Scala',\n",
              " 'Shell',\n",
              " 'The',\n",
              " 'easiest',\n",
              " 'way',\n",
              " 'to',\n",
              " 'start',\n",
              " 'using',\n",
              " 'Spark',\n",
              " 'is',\n",
              " 'through',\n",
              " 'the',\n",
              " 'Scala',\n",
              " 'shell:',\n",
              " '```bash',\n",
              " './bin/spark-shell',\n",
              " '```',\n",
              " 'Try',\n",
              " 'the',\n",
              " 'following',\n",
              " 'command,',\n",
              " 'which',\n",
              " 'should',\n",
              " 'return',\n",
              " '1,000,000,000:',\n",
              " '```scala',\n",
              " 'scala>',\n",
              " 'spark.range(1000',\n",
              " '*',\n",
              " '1000',\n",
              " '*',\n",
              " '1000).count()',\n",
              " '```',\n",
              " '##',\n",
              " 'Interactive',\n",
              " 'Python',\n",
              " 'Shell',\n",
              " 'Alternatively,',\n",
              " 'if',\n",
              " 'you',\n",
              " 'prefer',\n",
              " 'Python,',\n",
              " 'you',\n",
              " 'can',\n",
              " 'use',\n",
              " 'the',\n",
              " 'Python',\n",
              " 'shell:',\n",
              " '```bash',\n",
              " './bin/pyspark',\n",
              " '```',\n",
              " 'And',\n",
              " 'run',\n",
              " 'the',\n",
              " 'following',\n",
              " 'command,',\n",
              " 'which',\n",
              " 'should',\n",
              " 'also',\n",
              " 'return',\n",
              " '1,000,000,000:',\n",
              " '```python',\n",
              " '>>>',\n",
              " 'spark.range(1000',\n",
              " '*',\n",
              " '1000',\n",
              " '*',\n",
              " '1000).count()',\n",
              " '```',\n",
              " '##',\n",
              " 'Example',\n",
              " 'Programs',\n",
              " 'Spark',\n",
              " 'also',\n",
              " 'comes',\n",
              " 'with',\n",
              " 'several',\n",
              " 'sample',\n",
              " 'programs',\n",
              " 'in',\n",
              " 'the',\n",
              " '`examples`',\n",
              " 'directory.',\n",
              " 'To',\n",
              " 'run',\n",
              " 'one',\n",
              " 'of',\n",
              " 'them,',\n",
              " 'use',\n",
              " '`./bin/run-example',\n",
              " '<class>',\n",
              " '[params]`.',\n",
              " 'For',\n",
              " 'example:',\n",
              " '```bash',\n",
              " './bin/run-example',\n",
              " 'SparkPi',\n",
              " '```',\n",
              " 'will',\n",
              " 'run',\n",
              " 'the',\n",
              " 'Pi',\n",
              " 'example',\n",
              " 'locally.',\n",
              " 'You',\n",
              " 'can',\n",
              " 'set',\n",
              " 'the',\n",
              " 'MASTER',\n",
              " 'environment',\n",
              " 'variable',\n",
              " 'when',\n",
              " 'running',\n",
              " 'examples',\n",
              " 'to',\n",
              " 'submit',\n",
              " 'examples',\n",
              " 'to',\n",
              " 'a',\n",
              " 'cluster.',\n",
              " 'This',\n",
              " 'can',\n",
              " 'be',\n",
              " 'a',\n",
              " 'mesos://',\n",
              " 'or',\n",
              " 'spark://',\n",
              " 'URL,',\n",
              " '\"yarn\"',\n",
              " 'to',\n",
              " 'run',\n",
              " 'on',\n",
              " 'YARN,',\n",
              " 'and',\n",
              " '\"local\"',\n",
              " 'to',\n",
              " 'run',\n",
              " 'locally',\n",
              " 'with',\n",
              " 'one',\n",
              " 'thread,',\n",
              " 'or',\n",
              " '\"local[N]\"',\n",
              " 'to',\n",
              " 'run',\n",
              " 'locally',\n",
              " 'with',\n",
              " 'N',\n",
              " 'threads.',\n",
              " 'You',\n",
              " 'can',\n",
              " 'also',\n",
              " 'use',\n",
              " 'an',\n",
              " 'abbreviated',\n",
              " 'class',\n",
              " 'name',\n",
              " 'if',\n",
              " 'the',\n",
              " 'class',\n",
              " 'is',\n",
              " 'in',\n",
              " 'the',\n",
              " '`examples`',\n",
              " 'package.',\n",
              " 'For',\n",
              " 'instance:',\n",
              " '```bash',\n",
              " 'MASTER=spark://host:7077',\n",
              " './bin/run-example',\n",
              " 'SparkPi',\n",
              " '```',\n",
              " 'Many',\n",
              " 'of',\n",
              " 'the',\n",
              " 'example',\n",
              " 'programs',\n",
              " 'print',\n",
              " 'usage',\n",
              " 'help',\n",
              " 'if',\n",
              " 'no',\n",
              " 'params',\n",
              " 'are',\n",
              " 'given.',\n",
              " '##',\n",
              " 'Running',\n",
              " 'Tests',\n",
              " 'Testing',\n",
              " 'first',\n",
              " 'requires',\n",
              " '[building',\n",
              " 'Spark](#building-spark).',\n",
              " 'Once',\n",
              " 'Spark',\n",
              " 'is',\n",
              " 'built,',\n",
              " 'tests',\n",
              " 'can',\n",
              " 'be',\n",
              " 'run',\n",
              " 'using:',\n",
              " '```bash',\n",
              " './dev/run-tests',\n",
              " '```',\n",
              " 'Please',\n",
              " 'see',\n",
              " 'the',\n",
              " 'guidance',\n",
              " 'on',\n",
              " 'how',\n",
              " 'to',\n",
              " '[run',\n",
              " 'tests',\n",
              " 'for',\n",
              " 'a',\n",
              " 'module,',\n",
              " 'or',\n",
              " 'individual',\n",
              " 'tests](https://spark.apache.org/developer-tools.html#individual-tests).',\n",
              " 'There',\n",
              " 'is',\n",
              " 'also',\n",
              " 'a',\n",
              " 'Kubernetes',\n",
              " 'integration',\n",
              " 'test,',\n",
              " 'see',\n",
              " 'resource-managers/kubernetes/integration-tests/README.md',\n",
              " '##',\n",
              " 'A',\n",
              " 'Note',\n",
              " 'About',\n",
              " 'Hadoop',\n",
              " 'Versions',\n",
              " 'Spark',\n",
              " 'uses',\n",
              " 'the',\n",
              " 'Hadoop',\n",
              " 'core',\n",
              " 'library',\n",
              " 'to',\n",
              " 'talk',\n",
              " 'to',\n",
              " 'HDFS',\n",
              " 'and',\n",
              " 'other',\n",
              " 'Hadoop-supported',\n",
              " 'storage',\n",
              " 'systems.',\n",
              " 'Because',\n",
              " 'the',\n",
              " 'protocols',\n",
              " 'have',\n",
              " 'changed',\n",
              " 'in',\n",
              " 'different',\n",
              " 'versions',\n",
              " 'of',\n",
              " 'Hadoop,',\n",
              " 'you',\n",
              " 'must',\n",
              " 'build',\n",
              " 'Spark',\n",
              " 'against',\n",
              " 'the',\n",
              " 'same',\n",
              " 'version',\n",
              " 'that',\n",
              " 'your',\n",
              " 'cluster',\n",
              " 'runs.',\n",
              " 'Please',\n",
              " 'refer',\n",
              " 'to',\n",
              " 'the',\n",
              " 'build',\n",
              " 'documentation',\n",
              " 'at',\n",
              " '[\"Specifying',\n",
              " 'the',\n",
              " 'Hadoop',\n",
              " 'Version',\n",
              " 'and',\n",
              " 'Enabling',\n",
              " 'YARN\"](https://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn)',\n",
              " 'for',\n",
              " 'detailed',\n",
              " 'guidance',\n",
              " 'on',\n",
              " 'building',\n",
              " 'for',\n",
              " 'a',\n",
              " 'particular',\n",
              " 'distribution',\n",
              " 'of',\n",
              " 'Hadoop,',\n",
              " 'including',\n",
              " 'building',\n",
              " 'for',\n",
              " 'particular',\n",
              " 'Hive',\n",
              " 'and',\n",
              " 'Hive',\n",
              " 'Thriftserver',\n",
              " 'distributions.',\n",
              " '##',\n",
              " 'Configuration',\n",
              " 'Please',\n",
              " 'refer',\n",
              " 'to',\n",
              " 'the',\n",
              " '[Configuration',\n",
              " 'Guide](https://spark.apache.org/docs/latest/configuration.html)',\n",
              " 'in',\n",
              " 'the',\n",
              " 'online',\n",
              " 'documentation',\n",
              " 'for',\n",
              " 'an',\n",
              " 'overview',\n",
              " 'on',\n",
              " 'how',\n",
              " 'to',\n",
              " 'configure',\n",
              " 'Spark.',\n",
              " '##',\n",
              " 'Contributing',\n",
              " 'Please',\n",
              " 'review',\n",
              " 'the',\n",
              " '[Contribution',\n",
              " 'to',\n",
              " 'Spark',\n",
              " 'guide](https://spark.apache.org/contributing.html)',\n",
              " 'for',\n",
              " 'information',\n",
              " 'on',\n",
              " 'how',\n",
              " 'to',\n",
              " 'get',\n",
              " 'started',\n",
              " 'contributing',\n",
              " 'to',\n",
              " 'the',\n",
              " 'project.']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# filter: only words that begins with 'a'\n",
        "filterRdd = linesRdd.flatMap(lambda line: line.split()) \\\n",
        "                    .filter(lambda word: word.startswith('a'))\n",
        "\n",
        "filterRdd.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4K4cdm09v3_Y",
        "outputId": "8d874179-c0a4-43b5-fb8a-515409dbac58"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a',\n",
              " 'analytics',\n",
              " 'and',\n",
              " 'and',\n",
              " 'an',\n",
              " 'analysis.',\n",
              " 'also',\n",
              " 'a',\n",
              " 'and',\n",
              " 'and',\n",
              " 'a',\n",
              " 'and',\n",
              " 'a',\n",
              " 'available',\n",
              " 'at',\n",
              " 'an',\n",
              " 'also',\n",
              " 'also',\n",
              " 'a',\n",
              " 'a',\n",
              " 'and',\n",
              " 'also',\n",
              " 'an',\n",
              " 'abbreviated',\n",
              " 'are',\n",
              " 'a',\n",
              " 'also',\n",
              " 'a',\n",
              " 'and',\n",
              " 'against',\n",
              " 'at',\n",
              " 'and',\n",
              " 'a',\n",
              " 'and',\n",
              " 'an']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lista = ['um', 'um', 'dois', 'tres']\n",
        "rdd = spark.sparkContext.parallelize(lista)\n",
        "# total occurency of each word\n",
        "rdd2 = rdd.map(lambda w: (w, 1)) \\\n",
        "          .reduceByKey(lambda a,b: a+b)\n",
        "\n",
        "rdd2.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hp4xwUuEv4MP",
        "outputId": "8292f7fb-60d5-4e3f-c895-e66d1c8ebdcb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('um', 2), ('dois', 1), ('tres', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sort by key\n",
        "lista = ['um', 'um', 'dois', 'tres']\n",
        "rdd = spark.sparkContext.parallelize(lista)\n",
        "# sorting by word (key)\n",
        "rdd2 = rdd.map(lambda w: (w, 1)) \\\n",
        "          .reduceByKey(lambda a,b: a+b) \\\n",
        "          .sortByKey('asc')\n",
        "\n",
        "rdd2.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_LliKL-v4PL",
        "outputId": "792fe919-0db6-43c7-c638-b5f2d5508a55"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('dois', 1), ('tres', 1), ('um', 2)]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sort by\n",
        "lista = ['um', 'um', 'dois', 'tres']\n",
        "rdd = spark.sparkContext.parallelize(lista)\n",
        "# sorting by value (occurency0\n",
        "rdd2 = rdd.map(lambda w: (w, 1)) \\\n",
        "          .reduceByKey(lambda a,b: a+b) \\\n",
        "          .sortBy(lambda t: t[1])\n",
        "\n",
        "rdd2.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03fM1Zh7v4Sg",
        "outputId": "4f925e5c-7dfe-4853-94d3-24fd21d9f5cd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('dois', 1), ('tres', 1), ('um', 2)]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# union\n",
        "lista1 = ['um', 'um', 'dois', 'tres']\n",
        "lista2 = ['quatro', 'cinco']\n",
        "\n",
        "rdd1 = spark.sparkContext.parallelize(lista1)\n",
        "rdd2 = spark.sparkContext.parallelize(lista2)\n",
        "\n",
        "rddUnion = rdd1.union(rdd2)\n",
        "rddUnion.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38I2Rpggx4Ob",
        "outputId": "ece2ca28-f38d-46d2-88a3-1a55f846fe80"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['um', 'um', 'dois', 'tres', 'quatro', 'cinco']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# intersection\n",
        "lista1 = ['um', 'um', 'dois', 'tres']\n",
        "lista2 = ['um', 'quatro', 'cinco']\n",
        "\n",
        "rdd1 = spark.sparkContext.parallelize(lista1)\n",
        "rdd2 = spark.sparkContext.parallelize(lista2)\n",
        "\n",
        "rddIntersection = rdd1.intersection(rdd2)\n",
        "rddIntersection.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjOc2-dDx4Y_",
        "outputId": "45f7b439-9f42-4c3f-d8d9-d573e92801ea"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['um']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# distinct\n",
        "lista1 = ['um', 'um', 'dois', 'tres']\n",
        "\n",
        "rdd1 = spark.sparkContext.parallelize(lista1)\n",
        "\n",
        "rddDistinct = rdd1.distinct()\n",
        "rddDistinct.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4X0U6Bh4x4b5",
        "outputId": "d68a5e8a-3441-47b8-ce42-31a4fe496445"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['um', 'dois', 'tres']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# join\n",
        "lista1 = [('Pedro', 39), ('Maria', 30)]\n",
        "lista2 = [('Pedro', 'BH'), ('Maria', 'SP'), ('João', 'RJ')]\n",
        "\n",
        "rdd1 = spark.sparkContext.parallelize(lista1)\n",
        "rdd2 = spark.sparkContext.parallelize(lista2)\n",
        "\n",
        "rddJoin = rdd1.join(rdd2)\n",
        "rddJoin.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlFphuOzx4fc",
        "outputId": "9fd2667b-48a8-4e3a-ac8f-384c2a80a71a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Pedro', (39, 'BH')), ('Maria', (30, 'SP'))]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# action: the data are stored in memory\n",
        "rddJoin.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NE0FMwIjzsu7",
        "outputId": "0fbff04d-d06c-4abf-d2cf-6dfdaf19c64f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Pedro', (39, 'BH')), ('Maria', (30, 'SP'))]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# count\n",
        "rddJoin.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsMLkT6jz1eh",
        "outputId": "c3967fcc-4964-4561-d722-c200d5248850"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# take: take n random registers\n",
        "rddJoin.take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sk-jLv5hz1hy",
        "outputId": "15d56838-1203-4979-9c16-e3aa2b9ae04a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Pedro', (39, 'BH'))]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# top n registers\n",
        "lista1 = ['um', 'um', 'dois', 'tres']\n",
        "lista2 = ['um', 'quatro', 'cinco']\n",
        "\n",
        "rdd1 = spark.sparkContext.parallelize(lista1)\n",
        "rdd2 = spark.sparkContext.parallelize(lista2)\n",
        "\n",
        "rddIntersection = rdd1.intersection(rdd2)\n",
        "rddIntersection.collect()\n",
        "\n",
        "rddUnion.top(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2WugJU8z1lN",
        "outputId": "f02b34c2-8d67-4712-d8b8-25ddd86e2f19"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Pedro', (39, 'BH')), ('Maria', (30, 'SP'))]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# countByValue: count the times of occurency of the item\n",
        "lista1 = ['um', 'um', 'dois', 'tres']\n",
        "lista2 = ['um', 'quatro', 'cinco']\n",
        "\n",
        "rdd1 = spark.sparkContext.parallelize(lista1)\n",
        "rdd2 = spark.sparkContext.parallelize(lista2)\n",
        "\n",
        "rddIntersection = rdd1.intersection(rdd2)\n",
        "rddIntersection.collect()\n",
        "\n",
        "rddUnion.countByValue()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIQ2BIWQz1p_",
        "outputId": "85d39194-4097-49a6-95bd-b9cae87e0a90"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int, {('Pedro', (39, 'BH')): 1, ('Maria', (30, 'SP')): 1})"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saving result in a file\n",
        "rddUnion.saveAsTextFile('result.txt')"
      ],
      "metadata": {
        "id": "wW6IjPqFz1ub"
      },
      "execution_count": 27,
      "outputs": []
    }
  ]
}